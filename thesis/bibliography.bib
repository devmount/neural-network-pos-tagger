Automatically generated by Mendeley Desktop 1.13.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@thesis{mueller2015,
  author = {Andreas M\"uller},
  title  = {{Analyse von Wort-Vektoren deutscher Textkorpora}},
  school = {Technische Universit\"at Berlin},
  year   = 2015,
  month  = 7,
  type   = {Bachelor's Thesis},
  url    = {http://devmount.github.io/GermanWordEmbeddings/}
}
@article{Blei2012,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/blei03a.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
number = {4-5},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/~blei/lda-c/$\backslash$npapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993$\backslash$npapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A$\backslash$npapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6$\backslash$nhttp://www.crossref.org/jmlr},
volume = {3},
year = {2012}
}
@book{Minsky1969,
author = {Minsky, Marvin and Papert, Seymour},
publisher = {MIT Press},
title = {{Perceptron - An Essay in Computational Geometry}},
year = {1969}
}
@article{Deerwester1990,
abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or- thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re- turned. initial tests find this completely automatic method for retrieval to be promising.},
archivePrefix = {arXiv},
arxivId = {arXiv:1403.2923v1},
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
eprint = {arXiv:1403.2923v1},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/JASIS.lsi.90.pdf:pdf},
isbn = {9781450300322},
issn = {0002-8231},
journal = {Journal of the American Society for Information Science},
keywords = {automatic indexing,singular-value decomposition},
pages = {391--407},
pmid = {470195},
title = {{Indexing by latent semantic analysis}},
url = {http://doi.wiley.com/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
volume = {41},
year = {1990}
}
@techreport{Curtiss2013,
abstract = {Unicorn is an online, in-memory social graph-aware index- ing system designed to search trillions of edges between tens of billions of users and entities on thousands of commodity servers. Unicorn is based on standard concepts in informa- tion retrieval, but it includes features to promote results with good social proximity. It also supports queries that re- quire multiple round-trips to leaves in order to retrieve ob- jects that are more than one edge away from source nodes. Unicorn is designed to answer billions of queries per day at latencies in the hundreds of milliseconds, and it serves as an infrastructural building block for Facebook’s Graph Search product. In this paper, we describe the data model and query language supported by Unicorn. We also describe its evolution as it became the primary backend for Facebook’s search o↵erings.},
author = {Curtiss, Michael and Becker, Iain and Bosman, Tudor},
booktitle = {Proceedings of the \ldots},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Unicorn A System for Searching the Social Graph - facebook - p871-curtiss.pdf:pdf},
isbn = {2150-8097},
issn = {2150-8097},
pages = {1150--1161},
title = {{Unicorn: a system for searching the social graph}},
url = {http://dl.acm.org/citation.cfm?id=2536239},
year = {2013}
}
@article{Goodman2001,
abstract = {In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We find some significant interactions, especially with smoothing and clustering techniques. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38\% and 50\% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9\%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline. This is the extended version of the paper; it contains additional details and proofs, and is designed to be a good introduction to the state of the art in language modeling.},
archivePrefix = {arXiv},
arxivId = {cs/0108005},
author = {Goodman, Joshua},
doi = {10.1006/csla.2001.0174},
eprint = {0108005},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/A Bit of Progress in Language Modeling - Good01.pdf:pdf},
issn = {08852308},
pages = {73},
primaryClass = {cs},
title = {{A Bit of Progress in Language Modeling}},
url = {http://arxiv.org/abs/cs/0108005},
year = {2001}
}
@book{Bellman1961,
author = {Bellman, R.E.},
publisher = {Princeton University Press (Princeton, NJ)},
title = {{Adaptive control processes: A guided tour}},
year = {1961}
}
@article{Miners2013,
author = {Miners, Zach},
journal = {PCWorld},
title = {{Yahoo buys SkyPhrase to better understand natural language}},
url = {http://www.pcworld.com/article/2068240/yahoo-buys-skyphrase-to-better-understand-natural-language.html},
year = {2013}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/Rosenblatt1958.pdf:pdf},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological review},
keywords = {PERCEPTION},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@article{Mikolov2012,
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Linguistic Regularities in Continuous Space Word Representations rvecs.pdf:pdf},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
year = {2012}
}
@misc{Widrow1960,
author = {Widrow, Bernard},
booktitle = {Stanford Electronics Laboratories Technical Report},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/widrow1960anadaptive.pdf:pdf},
title = {{An Adaptive 'Adaline' Neuron Using Chemical 'Memistors'}},
year = {1960}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J J},
doi = {10.1073/pnas.79.8.2554},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/hopfield82.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {8},
pages = {2554--2558},
pmid = {6953413},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
volume = {79},
year = {1982}
}
@article{Arras2014,
address = {Berlin},
author = {Arras, Kheira Leila and Heyd, Jan and Ly, The-Anh and M\"{u}ller, Andreas},
file = {:C$\backslash$:/Users/Andreas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2014 - Exploring semantic word similarities in German News Articles.pdf:pdf},
institution = {Technische Universit\"{a}t Berlin},
title = {{Exploring semantic word similarities in German News Articles}},
year = {2014}
}
@article{Bender2002,
author = {Bender, R. and Ziegler, A. and Lange, St.},
file = {:C$\backslash$:/Users/Andreas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bender, Ziegler, Lange - 2002 - Logistische Regression.pdf:pdf},
number = {14},
pages = {12--14},
title = {{Logistische Regression}},
year = {2002}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Efficient Estimation of Word Representations in Vector Space 1301.3781.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@article{Zeng2014,
address = {China},
author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
file = {:C$\backslash$:/Users/Andreas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng et al. - 2014 - Relation Classification via Convolutional Deep Neural Network.pdf:pdf},
institution = {Institute of Automation, Chinese Academy of Sciences},
pages = {1--10},
title = {{Relation Classification via Convolutional Deep Neural Network}},
url = {http://www.nlpr.ia.ac.cn/cip/liukang.files/coling2014.pdf},
year = {2014}
}
@article{Shapiro2013,
author = {Shapiro, David and Platts, Doug and Martinez, Magico},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Google-Hummingbird-Explained-iCrossing-POV.pdf:pdf},
journal = {icrossing},
title = {{Google hummingbird explained}},
year = {2013}
}
@article{Bengio2003,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Jauvin, Christian},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/A Neural Probabilistic Language Model - bengio03a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@book{Harris1954,
author = {Harris, Zellig},
booktitle = {Word},
pages = {146--162},
title = {{Distributional Structure}},
volume = {1},
year = {1954}
}
@article{Socher2013,
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Recursive Deep Models for Semantic CompositionalityOver a Sentiment Treebank - EMNLP2013\_RNTN.pdf:pdf},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
url = {http://nlp.stanford.edu/sentiment/},
year = {2013}
}
@article{Brown1992,
author = {Brown, Peter F. and DeSouza, Peter V. and Mercer, Robert L. and {Della Pietra}, Vincent J. and Lai, Jenifer C.},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/Class-Based n-gram Models of Natural Language.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {1950},
title = {{Class-Based n-gram Models of Natural Language}},
year = {1992}
}
@misc{Rumelhart1986,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
file = {:C$\backslash$:/Users/Andreas/Uni/BA/material/history/Chap8\_PDP86.pdf:pdf},
isbn = {026268053X},
issn = {1-55860-013-2},
keywords = {Adaptive systems,Learning,Learning machines,Perceptrons,and Back propagation.,networks},
pages = {318--362},
title = {{Learning internal representations by error propagation}},
volume = {1},
year = {1986}
}
@article{Mikolov2010,
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:C$\backslash$:/Users/Andreas/data/uni/BA/material/history/mikolov\_interspeech2010\_IS100722.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Rong,
author = {Rong, Xin},
file = {:C$\backslash$:/Users/Andreas/Downloads/word2vec Parameter Learning Explained - w2vexp.pdf:pdf},
pages = {1--19},
title = {{word2vec Parameter Learning Explained}}
}
@article{Morin2005,
abstract = {In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.},
author = {Morin, Frederic and Bengio, Y},
file = {:C$\backslash$:/Users/Andreas/Downloads/Hierarchical Probabilistic Neural Network Language Model - hierarchical-nnlm-aistats05.pdf:pdf},
journal = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
pages = {246--252},
title = {{Hierarchical probabilistic neural network language model}},
url = {http://www.iro.umontreal.ca/labs/neuro/pointeurs/hierarchical-nnlm-aistats05.pdf$\backslash$nhttp://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf},
year = {2005}
}
@book{kucera1967,
author = {Ku\v{c}era, H and Francis, W N},
publisher = {Brown University Press},
title = {{Computational Analysis of Present-Day American English}},
url = {https://books.google.de/books?id=gb5ZAAAAMAAJ},
year = {1967}
}
@article{Bengio2007,
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
file = {:C$\backslash$:/Users/Andreas/data/uni/BA/material/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
journal = {Advances in neural information processing systems},
number = {1},
pages = {153},
title = {{Greedy Layer-Wise Training of Deep Networks}},
volume = {19},
year = {2007}
}
@article{Mikolov2013Dist,
archivePrefix = {arXiv},
arxivId = {arXiv:1310.4546v1},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {arXiv:1310.4546v1},
file = {:C$\backslash$:/Users/Andreas/data/uni/BA/material/Distributed Representations of Words and Phrases 1310.4546.pdf:pdf},
pages = {1--9},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Peng2003,
author = {Peng, F},
file = {:C$\backslash$:/Users/Andreas/data/uni/BA/material/Augmenting Naive Bayes Classifiers with Statistical Language Mode.pdf:pdf},
journal = {Computer Science Department Faculty Publication Series},
title = {{Augmentating Naive Bayes Classifiers with Statistical Language Models}},
url = {http://scholarworks.umass.edu/cs\_faculty\_pubs/91},
volume = {Paper 91},
year = {2003}
}
@article{Pearson1901,
author = {Pearson, Karl},
doi = {10.1080/14786440109462720},
file = {:C$\backslash$:/Users/Andreas/data/uni/BA/material/PCA pearson1901.pdf:pdf},
isbn = {1941-5982},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
pages = {559--572},
title = {{On lines and planes of closest fit to systems of points in space}},
url = {http://dx.doi.org/10.1080/14786440109462720},
volume = {2},
year = {1901}
}
