\input{thesis_preamble.tex} % import preamble config
% start document
\begin{document}
\pagenumbering{arabic} % große Römische Seitenummerierung
\pagestyle{empty}

% title page
\begin{center}
\includegraphics[width=0.28\textwidth]{images/logo_tu_berlin}
\vspace{8mm}

{\huge Technische Universität Berlin}\\
\vspace{2mm}
% {\large Quality and Usability Lab}\\
% \vspace{1mm}
{\large Institute of Software Engineering\\and Theoretical Computer Science}\\
\vspace{11mm}

{\Huge Part-of-Speech Tagging\\[-2mm] with Neural Networks\\[-2mm] for a Conversational Agent\\}
\vspace{20mm}
{\Huge \b{Master Thesis}}\\
{\b{Master of Science (M.Sc.)}}\\
\vspace{24mm}
\begin{tabular}{rl}
  \b{Author} & Andreas Müller\\
  \b{Major} & Computer Engineering\\
  \b{Matriculation No.} & 333471\\
   & \\
  \b{Date} & 18th May 2018 \\
  \b{1st supervisor} & Prof. Dr.-Ing. Sebastian Möller \\
  \b{2nd supervisor} & Dr. Axel Küpper \\
\end{tabular}

\end{center}
\clearpage
\pagestyle{scrheadings} % normale Kopf- und Fußzeilen für den Rest

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Eidesstattliche Erklärung}
Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Ausführungen, die anderen veröffentlichten oder nicht veröffentlichten Schriften wörtlich oder sinngemäß entnommen wurden, habe ich kenntlich gemacht.

Die Arbeit hat in gleicher oder ähnlicher Fassung noch keiner anderen Prüfungsbehörde vorgelegen.
\vspace{10mm}

Berlin, den \today\\

\vspace{1cm}
\rule{.5\textwidth}{.5pt}\\
Unterschrift

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Abstract}
A part-of-speech tagger is a system which automatically assigns the part of speech to words using contextual information. Potential applications for part-of-speech taggers exist in many areas of computational linguistics including speech recognition, speech synthesis, machine translation or information retrieval in general.

The part-of-speech tagging task of natural language processing is also used in the advisory artificial conversational agent called \Alex. \Alex\ was developed to answer questions about modules and courses at the Technische Universität Berlin. The system takes the written natural language requests from the user and tries to transform them into SQL-queries. To understand the natural language queries, the system uses a Hidden Markov Model (HMM) to assign tags to each word of the query (part-of-speech tagging). This HMM tagger is trained with manually created training templates that are filled with the data in the database to be queried. The manually created sentence-templates and the slot-filling resulted in many training data sentences with the same structure. This often led to wrong tagging results when the HMM tagger was presented with an input sentence, having a structure that doesn't occur in the training templates.

This thesis shows two different neural network approaches for the language modeling of the input sentences and evaluates and compares both neural network based tagger as well as the HMM based tagger.

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Zusammenfassung}
Ein Part-of-speech Tagger ist ein System, welches Wortarten anhand von Kontextinformationen automatisch den gegebenen Wörtern zuordnet. Potentielle Anwendungen solcher Tagger gibt es in vielen Bereichen der Computerlinguistik wie Spracherkennung, Sprachsynthese, maschinelle Übersetzung oder Information Retrieval im Allgemeinen.

Part-of-speech Tagging wird auch in \Alex\ verwendet, einem Artificial Conversational Agent. \Alex\ wurde entwickelt, um Fragen zu Modulen und Lehrveranstaltungen an der Technischen Universität Berlin zu beantworten. Das System nimmt die in natürlicher Sprache geschriebenen Anfragen des Benutzers und versucht diese in SQL-Abfragen umzuwandeln. Um die natürliche Sprache zu verstehen, verwendet das System ein Hidden-Markov-Model (HMM), um jedem Wort der Eingabe Wortarten zuzuweisen (Part-of-speech Tagging). Dieser HMM-Tagger wird mit manuell erstellten Trainingsvorlagen trainiert, die mit den Daten der abzufragenden Datenbank gefüllt werden. Die manuell erstellten Satzvorlagen führten zu vielen Trainingsdatensätzen mit gleicher Struktur und damit oft zu falschen Tagging-Ergebnissen, wenn der HMM-Tagger einen Eingabesatz mit einer Struktur verarbeiten sollte, die in den Trainingsvorlagen nicht vorkommt.

Diese Arbeit zeigt zwei verschiedene Ansätze für die Sprachmodellierung der Eingabesätze basierend auf neuronalen Netzwerken und bewertet und vergleicht sowohl die Neuronalen Netzwerk-basierten Tagger als auch den HMM-basierten Tagger.

% ===================================================================================
\BlankPage

% ===================================================================================
\tableofcontents

% ===================================================================================
\listoffigures

% ===================================================================================
\listoftables

% ===================================================================================
\chapter*{Abbreviations}\label{s.abbr}
\addcontentsline{toc}{chapter}{Abbreviations}
\markboth{Abbreviations}{Abbreviations}
\begin{acronym}[----------------]
 \acro{ACA}{\i{Artificial Conversational Agent}}
 \acro{ANN}{\i{Artificial Neural Network}}
 \acro{FNN}{\i{Feed-forward Neural Network}}
 \acro{HMM}{\i{Hidden Markov Model}}
 \acro{LSTM}{\i{Long Short-Term Memory}}
 \acro{NLP}{\i{Natural Language Processing}}
 \acro{NLTK}{\i{Natural Language Toolkit}}
 \acro{POS}{\i{Part-of-Speech}}
 \acro{RNN}{\i{Recurrent Neural Network}}
 \acro{SGD}{\i{Stochastic Gradient Descent}}
\end{acronym}

% ===================================================================================
\chapter{Introduction}\label{c.introduction}
% Turing: Artificial Intelligence
% Brown: NLP
% POS Tagging

\i{Learning} is one of the most essential parts of human life. From the beginning to the end, human beings acquire knowledge and skills. Learning means progress, additional value, failing and repeating. It enables growth and improvement.

In biology, learning is based on a specific strengthening of the connection of certain nerve cells in the central nervous system by facilitating signal transmission at the synapses through appropriate modifications. Being a huge amendable network of connected neurons the nervous system served as a role model for a research field called \i{Machine Learning}. This term was coined by A. Samuel\footnote{Arthur Lee Samuel was an early researcher in machine learning and artificial intelligence. He developed the first successful self-learning program: the Samuel-Checkers game \cite{samuel1959}.} \cite{samuel1959} in 1959, who distinguished two general approaches to the problem of machine learning: a general-purpose randomly connected Neural Network approach and a special-purpose highly organized network. Following a publication of  W. McCulloch about the comparison of a computer with the nervous system of a flatworm in 1949 \cite{mcculloch1949}, he stated:

\vspace{1em}
\i{``A comparison between the size of the switching nets\\that can be reasonably constructed or simulated at the present time\\and the size of the neural nets used by animals,\\suggests that we have a long way to go before we obtain practical devices.''}\\
\parbox{\textwidth}{\hfill \hfill -- Arthur Lee Samuel (1959)}
\vspace{.5em}

Less than 60 years later, today we have a lot of practical devices using machine learning and artificial intelligence technologies in our everyday life. Especially the processing and understanding of spoken or written natural language has a wide range of applications. One of those applications are advisory artificial conversational agents (ACA), chat-bots in short. They are designed to give natural language answers to natural language questions, making it as easy as possible for users to interact with a special system. \Alex\ is an example of an ACA that is able answer questions about courses and modules of the TU Berlin. This thesis aims to improve the understanding and learning of natural language of \Alex\ with artificial neural networks.

\section{Scope of this Thesis}\label{c.introduction.scope}
The scope of this thesis is the development of a neural network based part-of-speech tagger for the advisory Artificial Conversational Agent \Alex, the training of different language models and their evaluation with corresponding test sets.

In order to accomplish the new language models, two different neural network architectures are implemented: A feed-forward neural network and a recurrent neural network. For the training of both neural network implementations, a corpus of tagged language data is generated with the help various input templates, which are created on the basis of logged user input data.

To evaluate the language models, a data set of known data\footnote{Data, that was already used for the training of the model} and unknown data\footnote{Data, that includes words and sentence structures, that didn't occur in the training data sets} is created. On the basis of this evaluation, both neural network models and the HMM are compared to each other.

In accordance to the evaluation results, the former HMM based part-of-speech tagger is then replaced by this new tagger. To guarantee a seamless integration, the new tagger is implemented as a separate module with the same program interface the old tagger already utilizes. This way no other components of the conversational agent have to be changed and the effort of the replacement is kept minimal.

\section{Related Work}\label{c.introduction.related}
This thesis is build upon the work of T. Michael \cite{michael2016}, who describes the design and implementation of \Alex\ in detail. The conversational agent was implemented for the purpose of helping students of the TU Berlin to organize their studies by providing a simple way to gain information about modules and courses. It utilizes two separate already existing baseline systems by merging their data into one relational database. This database is  used as the central access point for the information that users want to retrieve.

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/alex_screencapture}
	\caption[User Interface of \Alex]{The user interface of \Alex. The left section contains the conversation with the Agent and a field where the user can type. The right section shows the result of the generated database query in tabular form.\\In this example, the user asked for all courses of the subject \i{computer science} that provide 9 ECTS and are scheduled on a Monday. The agent answered accordingly and provided a list of 9 courses that fulfill the conditions.\\This image was captured at the 21st April 2018.}
	\label{f.alex_ui}
\end{figure}

\Alex\ consists of several processing modules:

\begin{itemize}
	\item The \b{tagging module} uses a Hidden Markov Model to calculate the parts of speech for the user input, later described in chapter \ref{c.alex.hmm}
	\item The \b{query generation module} composes actual SQL queries from the tagging output data by recognizing the requested model and the return type
	\item The \b{filter extraction module} provides refinement and constraint handling for the query generator
	\item The \b{response generation module} formulates answers for the user input in natural language by processing the generated query, the recognized model and the conversation state.
\end{itemize}

Moreover \Alex\ provides a user interface which utilizes web technologies and can be accessed in a web browser. Figure \ref{f.alex_ui} shows the user interface where the user asked one question and the agent returned the result in tabular form and answered accordingly.

The focus of this thesis lies on the tagging module, as the main objective is to replace the Hidden Markov Model by Artificial Neural Networks.

\subsection{The Hidden Markov Model}\label{c.introduction.related.hmm}
The Hidden Markov Model (HMM) is a probabilistic finite state machine that solves classification problems in general. It uses the observable output data of a system to derive hidden information from it. Among other applications, HMMs are used especially for speech recognition tasks.

The preliminary work for HMMs was done by R. L. Stratonovich. He first described the conditional Markov processes in 1960 \cite{stratonovich1960} that were used in the following years to describe simple Markov Models and later Hidden Markov Models (see Baum et. al. \cite{baum1966}\cite{baum1967}). The latter became popular for solving the task of automatic recognition of continuous speech \cite{baker1975} along with other applications like pattern recognition in general, the analysis of biological sequences (e.g. DNA) \cite{bishop1986} and part-of-speech tagging \cite{kupiec1992}.

\subsection{The Artificial Neural Network Model}\label{c.introduction.related.nn}
Artificial Neural Networks are networks that process information inspired by biological nervous system. They consist of connected computational units typically arranged in different layers. Such a unit (also called \i{artificial neuron}) can make calculations based on its inputs and pass the result to the next connected units. These connections are weighted, so that the weight can be adjusted depending on the activity of the unit. Thus a model of the features of the input data can be created.

After preceding research by W. McCulloch, W. Pitts \cite{mcculloch1943} and D. Hebb \cite{shaw1986} about arithmetical learning methods inspired by the connections of neurons in the 1940s, M. Minsky built the first neural network learning machine called SNARC (\i{Stochastic Neural Analog Reinforcement Computer)}\cite{crevier1993} in 1951.

In the late 1950s, F. Rosenblatt developed the \i{Mark I Perceptron} computer and published a theorem of convergence of the perceptron\cite{rosenblatt1958} in 1958. He coined the term \i{perceptron} for an algorithm that was able to learn the assignment of input data to different classes. The perceptron represents a simple artificial neural network containing one single neuron at first\footnote{Chapters \ref{c.postagging.fnn} and \ref{c.postagging.rnn} explain the architecture of different neural network structures in detail}. F. Rosenblatt stated, that every function that is representable by the model can be learned with the proposed learning method. In 1960, B. Widrow presented the ADALINE\footnote{ADALINE is an acronym for Adaptive Linear Neuron} model of a neural network, where the input weights could already be adjusted by the learning algorithm \cite{widrow1960}.

A publication of M. Minsky and S. Papert \cite{minsky1969} in 1969 analyzed and exposed some significant limitations of the basic perceptron. They pointed out, that it is not possible to learn functions without linear separability (e.g. the exclusive-or problem). Due to these limitations and the fact, that the processing power of computers at that time was not sufficient for larger neural networks, the research interest in artificial neural networks decreased in the following years.

In 1982, J. Hopfield presented a previously described Neural Network with feedback (known as \i{Hopfield network}), that was able to solve optimization problems like the \i{Traveling Salesman Problem}\footnote{The problem of the traveling salesman or round trip problem: The order of places to be visited once should be chosen in such a way that the distance covered is minimal, whereby the last place is again the starting point (round trip).}. Neural Network approaches got more attention again, also because the first processors based on transistor technology (microprocessors) came onto the market in the early 1970s and replaced the previously used tube technology in the following years, which made computers smaller and cheaper and increased their processing capacity.

For the task of POS tagging, Neural Network models were now able to outperform HMM based tagger. H. Schmid created and trained a multilayer Feed-forward Neural Network in 1994 and was able to show, that it performed better than an HMM tagger \cite{schmid1994} at that time. In 2000, Ma et. al. run a series of comparative experiments that proved, that the results of a neural network tagger can be superior to those of statistical models like the HMM \cite{ma2000}.

\section{Structure of this Thesis}\label{c.introduction.structure}
As introduction, this first chapter gave a short overview about the subject of natural language processing and part-of-speech tagging in general.

The second chapter describes structure and functionality of the already existing ACA \Alex\ with the main focus on its language model and tagging interface.

Chapter \ref{c.postagging} explains the implementation of a part-of-speech tagging system with two different neural network approaches.

The training of the language models including the retrieval of the training data and tuning of the training parameter is described in Chapter \ref{c.training}.

Chapter \ref{c.evaluation} shows the evaluation of each language model with a generated test set and their comparison.

In conclusion the final Chapter \ref{c.conclusion} discusses and summarizes the evaluation results and gives an outlook on future work.

% ===================================================================================
\chapter{\Alex: Artificial Conversational Agent}\label{c.alex}
\i{Design and Implementation of an Advisory Artificial Conversational Agent} by T. Michael \cite{michael2016} provides a detailed and comprehensive description of \Alex\ as a compilation of different modules. This chapter focuses on components that are relevant for the language processing and therefore adapted during this thesis: The retrieval and processing of training data (section \ref{c.alex.data}), the Hidden Markov Model tagger (section \ref{c.alex.hmm}) and the tagging interface (section \ref{c.alex.tagging}).

\section{System Overview}\label{c.alex.overview}
The modular structure of \Alex\ allows the separation of different functions and therefore easier replaceability of certain functionalities. Besides a module crawler for current data retrieval of web content for the database and a frontend interface module, \Alex\ offers a tagging module. This module provides the training of a language model as well as the assignment of tags to the words of a given input sentence.

Figure \ref{f.alex.components} shows the original architecture of \Alex. The following components belong to the tagging module and are adapted or replaced (emphasized with an orange border in figure \ref{f.alex.components}) in this thesis:

\begin{itemize}
	\item The \b{HMM Training Data} is replaced by training data that was generated with improved training sentence templates (this will be explained in detail in chapter \ref{c.training.data})
	\item \b{Training Data Loading and Slot Filling} are used to generate the new training data
	\item The \b{HMM Tagger} is replaced by a Neural Network based tagger
	\item \b{Part of Speech Tagging} of input sentences is realized by the tagging function of the new tagger
\end{itemize}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/alex_components}
	\caption[Component Overview of \Alex]{Overview of all components of \Alex. The orange bordered parts are components that lie within the scope of this thesis and are adapted or replaced. Original figure by T. Michael \cite{michael2016}}
	\label{f.alex.components}
\end{figure}

\section{Training Data}\label{c.alex.data}
In order to teach \Alex\ to correctly assign tags to words depending on their context, appropriate training data is required. The training data for \Alex\ proposed by T. Michael consists of \tt{556,111} tagged sentences generated with \tt{72} manually created sentence templates.

A sentence template is a sentence that provides the structure of a possible tagged training sentence with a proper syntax for slot filling. It consists of either special placeholders for specific data from the database (e.g. module titles), inline choices (e.g. the same sentence with every day of the week) or a marker to simply duplicate the sentence. The different slot filling forms can be combined or used multiple times in one sentence\footnote{T. Michael describes the training data structure in detail in chapter 3.4.2 of \i{Design and Implementation of an Advisory Artificial Conversational Agent} \cite{michael2016}.}.

For the training of the Neural Network Models in this thesis, this slot filling mechanism is adopted and improved for the training with Artificial Neural Networks (see chapter \ref{c.training}).

\section{The Hidden Markov Model Tagger}\label{c.alex.hmm}
As described in section \ref{c.introduction.related.hmm} \ of the introduction, the Hidden Markov Model (HMM) is a statistical tool that uses observable output data of a system to derive hidden information from it. Applications are image processing, gesture recognition and natural language processing tasks like speech recognition and part-of-speech tagging in particular.

In case of POS tagging, the observable states of the HMM represent the given sequence of words whereas the hidden states represent the corresponding parts of speech. The HMM calculates the joint probability of the whole sequence of hidden states with the help of transmission and output probabilities. Subsequently it finds the maximum probability of all possible state sequences and decides as a result, which parts of speech are most likely applied to the words of the input sequence.

Figure \ref{f.hmm_structure} illustrates an example of a state sequence with three hidden states (part of speech tags) and the observed word sequence in an HMM. The calculation of the joint probability $P$ of the word sequence in this case is shown in equation \ref{e.hmm_joint_probability} as the product of transmission and output probabilities.

\begin{equation}
    P = p_{start}\cdot p_{out,1}\cdot p_{trans,1}\cdot p_{out,2}\cdot p_{trans,2}\cdot p_{out,3} \label{e.hmm_joint_probability}
\end{equation}


\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/hmm_structure}
	\caption[Structure of a Hidden Markov Model]{An example of a state sequence of three hidden states (tag1 -- tag3) and an observed sequence of three words (word1 -- word3) in an Hidden Markov Model. $p_{start}$ denotes the start probability, $p_{trans}$ the transmission probabilities between hidden states and $p_{out}$ the output probabilities between a hidden state and an output.}
	\label{f.hmm_structure}
\end{figure}

For this purpose, an HMM is included in \Alex. According to T. Michael \cite{michael2016}, a tagging scheme was developed to extract exactly the information from a user input that is needed to successfully create a database query and return the information the user asked for. This tagging scheme was intentionally built domain independent, giving the opportunity to make \Alex\ an ACA for any topic providing a corresponding database and training data.

To maintain this universal applicability as well as the compatibility to other modules of \Alex, the Neural Network approaches presented in this thesis use the same tagging scheme \Alex\ already utilizes. For a better understanding of the evaluation results in Chapter \ref{c.evaluation}, table \ref{t.tagging_scheme} gives an overview of the 6 different classes of tags that are used by \Alex.

\begin{table}[H]
	\small\def\arraystretch{1.5}\begin{tabular}{ p{2mm} L{48mm} p{35mm} p{40mm} }
	\trule
	 & \textsc{Formats} & \textsc{Description} & \textsc{Example} \\
	\drule
	\tt{R} & \i{Return-tags}, describing data that is expected to be returned & \tt{R\_LIST} \newline \tt{R\_SINGLE} \newline \tt{R\_COUNT} & \i{"\b{Which} modules \dots"} \newline \i{"\b{Which} module \dots"} \newline \i{"\b{How many} modules \dots"} \\
	\mrule
	\tt{M} & \i{Model-tags}, describing the database model, e.g. \tt{M\_MTSModule} or \tt{M\_Course} & \tt{M\_[MODEL]} & \i{"Which \b{modules} \dots"} \newline \i{"Which \b{courses} \dots"} \\
	\mrule
	\tt{C} & \i{Constraint-tags}, filtering the result set, given a database model and corresponding field, e.g. \tt{C\_MTSModule:ects} & \tt{C\_[MODEL]:[FIELD]} & \i{"Modules with \b{6} ects \dots"} \\
	\mrule
	\tt{P} & \i{Property-tags}, indicating to include fields in the result set, e.g. \tt{P\_MTSModule:ects} & \tt{P\_[MODEL]:[FIELD]} & \i{"Modules with 6 \b{ects} \dots"} \\
	\mrule
	\tt{Q} & \i{Comparison-tags}, describing an equal, greater than or less than constraint & \tt{Q\_EQ} \newline \tt{Q\_LT} \newline \tt{Q\_GT} &  \i{"\dots\ with \b{exactly} 6 ects \dots"} \newline \i{"\dots\ \b{less than} 6 ects \dots"} \newline \i{"\dots\ \b{more than} 6 ects \dots"} \\
	\mrule
	\tt{X} & \i{Extra-tags}, describing words that are not relevant for the database query\tablefootnote{This can be either words with no special meaning at all (tagged with \tt{X}), or words that have no meaning for the database query but for the system itself (e.g. the tag \tt{X\_HELP} for the word \i{"help"}) or words that lead to a particular constraint (like the tag \tt{X\_Person:fullname} for the word \i{"Professor"}, that leads to a name).} & \tt{X} \newline \tt{X\_[WORD]} \newline \tt{X\_[MODEL]:[FIELD]} & \i{"\b{and}"}, \i{"\b{of}"}, \i{"\b{is}"} \newline \i{"I need \b{help}"} \newline \i{"\b{Professor} John Doe"} \\
	\brule
	\end{tabular}
	\caption[Tagging Scheme Overview]{Overview of the tagging scheme used in \Alex, consisting of 6 different classes of tags with a total of 12 different formats. The examples contain \b{emphasized} words that belong to the corresponding tag format. Detailed explanation of the tagging classes and its formats is given by T. Michael \cite{michael2016}.}
	\label{t.tagging_scheme}
	\vspace{1ex}
\end{table}

\section{Tagging Interface}\label{c.alex.tagging}
As described in the previous chapter, the implementation of the tagging module of \Alex\ utilizes a Hidden Markov Model for the part-of-speech tagging. \Alex\ uses an already existing implementation of the HMM Tagger from the Natural Language Toolkit (NLTK)\footnote{The Natural Language Toolkit is a collection of \i{Python} programming libraries for natural language processing, see \link{http://nltk.org}}, called \tt{HiddenMarkovModelTagger}.

To replace the existing tagger, a new tagger has to provide a class with two methods: \tt{train} and \tt{tag}. These methods are used to create the language model and apply it to unknown data.

The \tt{train} method creates a new instance of the tagger class, trains this class with the given training data and returns it. The training data itself must be a list of sentences, where a sentence is a list of tuples, containing each word of this sentence and its corresponding tag. The following exemplifies the structure of the training input data containing two sentences where each word is tagged with \i{TAG}:

\lstinputlisting[language=JSON, label={l.trainingdata}]{listings/method-train.example}

The \tt{tag} method attaches a tag to each word of an input sentence, according to the previously trained language model. The input has to be an unknown sentence as a simple list of words:

\lstinputlisting[language=JSON]{listings/method-tag-input.example}

The output is a corresponding list of tuples containing a word and its assigned tag:

\lstinputlisting[language=JSON]{listings/method-tag-output.example}


% ===================================================================================
\chapter{Part-of-Speech Tagging with Neural Networks}\label{c.postagging}
Chapter\ref{c.introduction.related.nn} introduced Neural Networks as a possible approach for POS tagging. The current chapter presents two different Neural Network architectures and their implementation, that is later used to train and evaluate language models with the objective of comparing their POS tagging accuracy with the accuracy of the HMM tagger, \Alex\ uses.

In general a Neural Network represents a mathematical function, that is able to assign specific output data to corresponding input data. This assignment is learned by processing input data whose output is known. Thus, it belongs to the category of supervised learning\footnote{\i{Supervised Learning} describes the process of gaining knowledge with the help of labeled data. Depending on the capability of the supervised learning algorithm to generalize from the given data, this knowledge can then be applied to unknown data.}.

The network itself emerges from the interconnection of nodes (the artificial neurons), that are usually arranged in different layers. Each node represents a non-linear\footnote{The non-linearity is important because linear functions cannot describe some mutual exclusive feature combinations.} activation function, that calculates an output depending on the sum of its inputs. Possible activation functions are the sigmoid function, hyperbolic tangent or the maximum function.

The training effect is achieved by weighting the connections of the nodes and adjust these weights during training. The weights of Neural Networks are typically represented as real numbers. The adjustments are defined by a learning algorithm that utilizes a particular learning method. Using the initial or current weights of the network, an error (also called \i{loss}) between the prediction and the given label can be computed on the last layer (the output layer) with the help of a corresponding loss function. The training objective is to find the minimum of the loss function of the current state of the network and adjust the weights accordingly. This can be achieved by using \i{backpropagation} with \i{stochastic gradient descent}.

The POS tagging task demands the processing of words which are represented as character strings. For easier handling, each word is mapped to an integer value (the word id).

\section{Feed-forward Neural Network Model}\label{c.postagging.fnn}
Feed-forward Neural Networks are a common type of ANNs, that consist of an input and an output layer with one or more hidden layers in between. An FNN propagates information through the network only in the forward direction, from the input to the output layer.

\subsection{Architecture}\label{c.postagging.fnn.architecture}
The FNN architecture that is used in this thesis is shown in figure \ref{f.fnn.structure}. The current word and the configured number of preceding words are each converted into word embeddings. These embeddings are concatenated to one vector, called the feature vector, which serves as input layer for the neural network. The FNN contains one hidden layer, which is connected to the input layer with weight matrix \tt{V}. The output layer represents the set of existing POS tags and is connected to the hidden layer with weight matrix \tt{W}.

For each word of every sentence of the training corpus, a feature vector is built and the data is sequentially propagated to the hidden and the output layer, predicting a POS tag for it and calculating the current loss and accuracy. After propagating the error back to adjust the weights and reduce the loss, the next training step is executed.

\begin{figure}[ht]
	\includegraphics[width=\textwidth]{images/fnn_structure}
	\caption[Structure of a Feed-forward Neural Network]{The structure of a feed-forward neural network. The feature vector is built by the embeddings of the corresponding input word on position \tt{p} and by its \tt{3} preceding words (here as an example, the number of predecessors is of course variable).}
	\label{f.fnn.structure}
\end{figure}

\subsection{Implementation}\label{c.postagging.fnn.implementation}
To implement the presented FNN architecture, the open source machine learning framework \i{TensorFlow}\footnote{TensorFlow is a library written in Python, that is utilized for high performance numerical computations especially for the machine learning domain. In this thesis, TensorFlow version 1.8 is used.\\See the official TensorFlow website: \link{https://www.tensorflow.org}} is used. In the following descriptions, all functions starting with \tt{tf} are functions from the TensorFlow library.

The nodes of the input layer are populated by the feature vector. To create the feature vector, an embedding matrix with the dimensions of vocabulary size $\times$ embedding size is created and initially filled with random normal distributed values\footnote{The generated values follow a normal distribution with mean of 0 and standard deviation of 0.1, except that values whose magnitude is more than 2 standard deviations from the mean are dropped (truncated) and re-picked. See the TensorFlow documentation.}, using the \tt{tf.truncated\_normal()} function. This matrix is used to retrieve the vectors for the current word and its predecessors via \tt{tf.nn.embedding\_lookup} and reshape them to one single vector, the feature vector. Figure \ref{f.fnn.feature} illustrates the creation of the feature vector.

\begin{figure}[ht]
	\vspace{1.5em}
	\hspace{-1.5em}\includegraphics[width=1.1\textwidth]{images/feature_vector}
	\caption[Creation of the feature vector]{An example of the creation of a feature vector for the FNN using \tt{3} past words and the current word.}
	\label{f.fnn.feature}
	\vspace{.5em}
\end{figure}

The hidden layer nodes are initialized with random normal distributed values as well. Their outputs are computed with the rectified linear activation function (the \i{rectifier}), utilizing the \tt{tf.nn.relu()} function. These nodes are therefore also called \i{rectified linear units} (RELUs).

Finally the values for the output layer (the logits) are computed with a matrix multiplication of the outputs of the RELUs and the weight matrix \tt{W} using the \tt{tf.matmul()} function. Now the loss can be calculated by computing the sparse softmax cross entropy between input labels and logits, using the \tt{tf.nn.sparse\_softmax\_cross\_entropy\_with\_logits()} function.

The predictions are calculated with the \tt{tf.argmax()} functions, that reduces the logits to the largest value, which represents the predicted POS tag. The function \tt{tf.train.AdamOptimizer()}\footnote{The Adam algorithm is an optimizer that was proposed by D. Kingma et. al. in 2014 \cite{kingma2014}.} enables backpropagation with stochastic gradient descent to optimize the computations and results during training.


\section{Recurrent Neural Network Model}\label{c.postagging.rnn}
Similar to the FNNs, the recurrent Neural Networks also consist of input, hidden and output layer. The basic difference is, that the information are not only linearly propagated forward, but information from precious training steps are also taken into account.

\subsection{Architecture}\label{c.postagging.rnn.architecture}
Figure \ref{f.rnn.structure} shows the architecture of an RNN, that was implemented for this thesis. Unlike the FNN, the RNN uses only the word ids as input features and doesn't utilize word embeddings. It contains one hidden layer, which is fed by the current input word on the one hand and by the output of the hidden layer of a previous training step on the other hand. This makes the RNN capable of memorizing information from the past. This capability can also be described as a long short-term memory (\i{LSTM}), emphasized in orange in figure \ref{f.rnn.structure}. The output layer represents the set of existing POS tags and is connected to the hidden layer with weight matrix
\tt{W}.

Each word of every sentence of the training corpus represents an input feature, which is together with information from previous training steps propagated to the hidden and the output layer. Similar to the FNN, the POS tag is predicted for the current word and the current loss and accuracy are calculated. After propagating the error back to adjust the weights and reduce the loss, the next training step is executed, taking the current training step into account.

\begin{figure}[ht]
	\includegraphics[width=\textwidth]{images/rnn_structure}
	\caption[Structure of a Recurrent Neural Network]{The structure of a recurrent neural network. The feature vector is the initial vector of the corresponding input word at time \tt{t}. The output of the hidden layer from previously trained words (here as an example at time \tt{x}) is fed back into the same hidden layer for the current word.}
	\label{f.rnn.structure}
\end{figure}

\subsection{Implementation}\label{c.postagging.rnn.implementation}
The implementation of the proposed RNN architecture utilizes the TensorFlow library as well. Next to the word id of the current word, the input layer has an additional dimension describing the number of past training steps that are considered for the current training step.

The hidden layer is build with \tt{tf.nn.rnn\_cell.LSTMCell()}\footnote{This function uses an implementation proposed by S. Hochreiter et. al. in 1997 \cite{hochreiter1997}.}, a function that creates a LSTM recurrent network cell which is then used in \tt{tf.nn.dynamic\_rnn()} to compute the logits for the output layer.

Loss, predictions and accuracy are implemented similar to the FNN. Also the same optimizer is used.

% ===================================================================================
\chapter{Training of Language Models}\label{c.training}
To obtain an optimal language model with the Neural Network approach proposed in this thesis, as much annotated training data as possible is required. The following sections describe the generation of tagged training data and the training of different language models due to parameter variation based on this generated training corpus.

\section{Training Data Corpus}\label{c.training.data}
To create a training corpus with tagged sentences, the already existing sentence template set from \Alex\ is used as the basis for an improved and extended template set. In addition, a log of user input data\footnote{The log data that was available for this thesis started on 30th of August, 2017 until 27th of April, 2018 and contained \tt{1293} entries of user input.} provides useful information about possible input sentences, that are not yet considered in the template set.

As described in chapter \ref{c.alex.data}, the HMM tagger of \Alex\ uses \tt{72} sentence templates to generate annotated training data. However the distribution of the generated sentences is highly unbalanced. Due to the combination of placeholders for different database fields and models, that are semantically meaningless, a huge number of training sentences exist, that are only partially suitable for training.

An analysis of the generated training corpus that was used to train the HMM tagger showed, that one single template out of the \tt{72} sentence templates created more than \tt{84\%} of the whole training corpus. This sentence template is the following\footnote{The corresponding English translation of this sentence template is: \i{Which modules are held by Professor <firstname> <lastname>}}:

\vbox{\begin{Verbatim}[fontsize=\scriptsize]
Welche   Module  werden von   Prof   {Person:firstname} {Person:lastname} angeboten
\end{Verbatim}
\vspace{-6mm}
{\color{gray}\begin{Verbatim}[fontsize=\scriptsize]
R_LIST M_MTSModule  X    X  X_Person C_Person:fullname  C_Person:fullname     X
\end{Verbatim}
}}

It combines all first names and all last names of each existing person in the database, leading to a huge number of name combinations, that do not exist.

Another example is the combination of study program degrees and program names. This combination exists in \tt{5} of the \tt{72} sentence templates, whereby all degrees are assigned to all study programs \tt{5} times, although not every combination exists in reality. The following is an example of one of this sentence templates\footnote{The corresponding English translation of this sentence template is: \i{Which modules can I attend to in <program-degree> <program-name>}}:

\vbox{\begin{Verbatim}[fontsize=\scriptsize]
Welche   Module    kann ich  im  {Program:degree}  {Program:name} belegen
\end{Verbatim}
\vspace{-6mm}
{\color{gray}\begin{Verbatim}[fontsize=\scriptsize]
R_LIST M_MTSModule  X    X   X   C_Program:degree  C_Program:name    X
\end{Verbatim}
}}

To address this issue, the following adjustments for the new template set were made:

\begin{itemize}
	\item Because of the very low occurrence in the log database, the slot \tt{\{Person:firstname\}} was removed completely from that sentence template, which limits the user input to last names only\footnote{This might seem like a degradation but is justified by the fact, that only \tt{3} of \tt{1293} log entries (\tt{0.2\%}) even contained a first name, always followed by the last name. In this specific use case, where all Persons are Professors and Lecturers, only using the last name appears legit.}
	\item the slot \tt{\{Program:degree\}} was partly replaced by the inline choice \tt{(bachelor|master|diplom)}, which were the main terms asked for in the logs.
\end{itemize}

Also the wording occurring in the logs referring to linking words and actions was improved with the help of inline choices. The following table shows those improvements as a comparison of the former and the new sentence template:

\begin{table}[H]
	\small\def\arraystretch{1.5}\begin{tabular}{ L{43mm} L{43mm} p{43mm} }
	\trule
	\textsc{Former Template} & \textsc{New Template} & \multicolumn{1}{L{43mm}}{\textsc{English Equivalent}} \\
	\drule
	Alle Module \b{vom} \{Program:degree\} \{Program:name\} & Alle Module \b{(vom|von|in|im)} \{Program:degree\} \{Program:name\} & \multicolumn{1}{L{43mm}}{\i{All modules \b{(of|in)} \{Program:degree\} \{Program:name\} }} \\
	\mrule
	Welche Module werden von Prof \{Person:firstname\} \{Person:lastname\} \b{angeboten} & Welche Module werden von Professor \{Person:lastname\} \b{(unterrichtet| angeboten|gehalten)} & \multicolumn{1}{L{43mm}}{\i{Which modules are \b{(taught|offered|held)} by Professor \{Person:lastname\} }} \\
	\mrule
	\b{Wieviele} LP \b{hat} das Modul \{MTSModule:title\} & \b{(Wieviele|Wieviel)} LP \b{(hat|bringt)} das Modul \{MTSModule:title\} & \multicolumn{1}{L{43mm}}{\i{\b{How many} ects does the module \{MTSModule:title\} \b{(have|bring)} }} \\
	\mrule
	\b{Informationen} \b{zu} Modul {MTSModule:title} & \b{(Informationen|Details| Mehr)} \b{(zu|zum)} Modul \{MTSModule:title\} & \multicolumn{1}{L{43mm}}{\i{\b{(Information|details| more)} \b{of} the module \{MTSModule:title\} }} \\
	\brule
	\end{tabular}
	\caption[Sentence Template Improvements]{An excerpt of the extension and improvement of the sentence templates by using inline choices. The last column provides the corresponding English translation for the new sentence template.}
	\label{t.improved_sentence_templates}
	\vspace{1em}
\end{table}

At last \tt{28} new sentence templates were added, that provide a sentence structure that was found in the logs but not in the former template set. The new set contains a total of \tt{100} sentence templates (including single word templates) and can be found in the appendix \ref{c.appendix.sentencetemplates}.

The resulting training data corpus used in this thesis now contains \tt{218.700} tagged sentences consisting of \tt{2.038.490} words. The size of the vocabulary is \tt{9141} words.

\section{Parameter Tuning}\label{c.training.tuning}
In order to achieve better evaluation results and therefore more correctly tagged words, different training parameters are varied. To see the effect of the training parameters on the accuracy of the resulting language model, one parameter was altered while keeping all other parameters constant.

For the training of the FNN models, the following \tt{4} parameters were considered:

\begin{itemize}
	\item \b{Number of past words} (\tt{\b{p}}) specifies, how many preceding words should be considered for the training of the current word
	\item \b{Embedding size} (\tt{\b{e}}) describes the dimension of the word embeddings, that were created for each word of the vocabulary during the training
	\item \b{Hidden layer size} (\tt{\b{s}}) characterizes the dimension of the hidden layer, meaning the number of neurons that the hidden layer consists of
	\item \b{Number of training epochs} (\tt{\b{n}}) indicates, how often the whole training corpus is processed during training
\end{itemize}

Based on these \tt{4} parameters, table \ref{t.training.tuning.fnn} shows different parameter combinations. The emphasized cells show the respective parameter that is variable. The first \tt{4} training groups represent the basic training to see the effect of each parameter in general. The subsequent training groups were additional parameter combinations, taking the evaluation results of the first training groups into account.

The resulting number of models is \tt{66}, however due to overlapping of configurations of the training groups, the total number of distinct models to be trained with the FNN is \tt{62}.

\begin{table}[H]
	\vspace{2em}
	\centering\small\def\arraystretch{1.5}\begin{tabular}{ c c c c c }
	\trule
	\tt{p} & \tt{e} & \tt{s} & \tt{n} & \textsc{Models} \\
	\drule
	\cellcolor{orange}\color{white}\b{\tt{0}--\tt{12}} & \tt{50} & \tt{100} & \tt{1} & \tt{13} \\
	\mrule
	\tt{1} & \cellcolor{orange}\color{white}\b{\tt{1}, \tt{5}, \tt{10}, \tt{25}, \tt{50}--\tt{350}\tablefootnote{With a step size of \tt{50}\label{fifty}}} & \tt{100} & \tt{1} & \tt{11} \\
	\mrule
	\tt{1} & \tt{50} & \cellcolor{orange}\color{white}\b{\tt{10}, \tt{25}, \tt{50}--\tt{600}\footref{fifty}} & \tt{1} & \tt{14} \\
	\mrule
	\tt{1} & \tt{50} & \tt{100} & \cellcolor{orange}\color{white}\b{\tt{1}, \tt{5}, \tt{10}, \tt{20}--\tt{140}\tablefootnote{With a step size of \tt{20}}} & \tt{10} \\
	\srule
	\tt{1} & \cellcolor{orange}\color{white}\b{\tt{200}--\tt{600}\footref{fifty}} & \tt{350} & \tt{1} & \tt{9} \\
	\mrule
	\tt{1} & \tt{250} & \cellcolor{orange}\color{white}\b{\tt{200}--\tt{600}\footref{fifty}} & \tt{1} & \tt{9} \\
	\brule
	\end{tabular}
	\caption[Parameter combinations of FNN Models]{The parameter configuration for the training of FNN models. The rows represent the training groups with the corresponding variable parameter (orange cells), while the columns represent the single training parameters (\tt{\b{p}} (Number of past words), \tt{\b{e}} (Embedding size), \tt{\b{s}} (Hidden layer size), \tt{\b{n}} (Number of training epochs)). The last column shows the resulting number of models for each training group.}
	\label{t.training.tuning.fnn}
	\vspace{1em}
\end{table}

For the training of the RNN models, the following \tt{3} parameters were considered:

\begin{itemize}
	\item \b{Number of time steps} (\tt{\b{t}}) specifies, how many past training steps should be considered for the training of the current word
	\item \b{Hidden layer size} (\tt{\b{s}}) characterizes the dimension of the hidden layer, meaning the number of neurons that the hidden layer consists of
	\item \b{Number of training epochs} (\tt{\b{n}}) indicates, how often the whole training corpus is processed during training
\end{itemize}

Based on these \tt{3} parameters, table \ref{t.training.tuning.rnn} shows different parameter combinations. The emphasized cells show the respective parameter that is variable. The resulting number of models is \tt{18}, however due to overlapping of configurations of the training groups, the total number of models to be trained with the FNN is \tt{16}.

\begin{table}[ht]
	\vspace{2em}
	\centering\small\def\arraystretch{1.5}\begin{tabular}{ c c c c c }
	\trule
	\tt{t} & \tt{s} & \tt{n} & \textsc{Models} \\
	\drule
	\cellcolor{orange}\color{white}\b{\tt{0}--\tt{10}} & \tt{50} & \tt{10} & \tt{10} \\
	\mrule
	\tt{1} & \cellcolor{orange}\color{white}\b{\tt{10}, \tt{25}, \tt{50}, \tt{100}} & \tt{10} & \tt{4} \\
	\mrule
	\tt{1} & \tt{50} & \cellcolor{orange}\color{white}\b{\tt{1}, \tt{5}, \tt{10}, \tt{20}} & \tt{4} \\
	\brule
	\end{tabular}
	\caption[Parameter combinations of RNN Models]{The parameter configuration for the training of RNN models. The rows represent the training groups with the corresponding variable parameter (orange cells), while the columns represent the single training parameters (\tt{\b{t}} (number of time steps), \tt{\b{s}} (hidden layer size), \tt{\b{n}} (number of training epochs)). The last column shows the resulting number of models for each training group.}
	\label{t.training.tuning.rnn}
\end{table}

% ===================================================================================
\chapter{Evaluation and Comparison}\label{c.evaluation}
After giving a short description of how the evaluation tests were created, this chapter provides the evaluation results of all models that were trained according to the previously described variation of training parameters. Also for each training group the evaluation results are presented and a comparison between the three different architectures (FNN, RNN and HMM) is shown.

\section{Test Design}\label{c.evaluation.test}
Two evaluate the trained language models, two test sets were designed: A test set containing a selection of tagged sentences as they exist in the training corpus (here called the \i{known test}) and a test set, where the structure of the sentences from the known test was modified in a way, that it still remains semantically meaningful but does not occur in the training corpus (called the \i{unknown test}). The unknown test deliberately contains words, that do not exist in the vocabulary of the training corpus to evaluate, how the model handles completely unknown data.

The sentences are divided into different topics, to cover a wide range of possible input data. Attention was paid to a balanced number of sentences in the different areas, to ensure, that the evaluation result doesn't depend on only one or two topics.

The tables \ref{t.evaluation_topics_known} and \ref{t.evaluation_topics_unknown} show the different topics for the known and the unknown test set, their respective number of evaluation sentences and one sentence as an illustration for each topic. Both test sets combined offer a total number of \tt{1058} tagged test sentences, consisting of \tt{7678} word-tag tuples.

\begin{table}[!ht]
	\centering\small\def\arraystretch{1.2}\begin{tabular}{ p{18mm} c L{95mm} }
	\toprule
	\textsc{Topic} & \textsc{Sentences} & \multicolumn{1}{L{95mm}}{\textsc{Example}} \\
	\midrule
	\midrule
	ECTS & \tt{44} & \multicolumn{1}{L{95mm}}{\i{Which modules have 6 ECTS}} \\
	\midrule
	Time & \tt{60} & \multicolumn{1}{L{95mm}}{\i{Which courses are on Tuesday}} \\
	\midrule
	Faculty & \tt{56} & \multicolumn{1}{L{95mm}}{\i{All modules of faculty 4}} \\
	\midrule
	Participants & \tt{54} & \multicolumn{1}{L{95mm}}{\i{Which courses are limited to 30 participants}} \\
	\midrule
	Persons & \tt{61} & \multicolumn{1}{L{95mm}}{\i{Which courses are taught by Professor Martinelli}} \\
	\midrule
	Program & \tt{60} & \multicolumn{1}{L{95mm}}{\i{I'm searching for all modules of the computer science program }} \\
	\midrule
	Modules & \tt{80} & \multicolumn{1}{L{95mm}}{\i{Modules with the title Cognitive Algorithms}} \\
	\midrule
	Chair & \tt{50} & \multicolumn{1}{L{95mm}}{\i{All modules of institute Media Science}} \\
	\midrule
	Exam & \tt{32} & \multicolumn{1}{L{95mm}}{\i{Courses with Group Lecture as examination}} \\
	\midrule
	Courses & \tt{45} & \multicolumn{1}{L{95mm}}{\i{When does Internet Security take place}} \\
	\midrule
	Locations & \tt{42} & \multicolumn{1}{L{95mm}}{\i{All courses in room Mainbuilding H 1012}} \\
	\bottomrule
	 & \tt{584} & \\
	\end{tabular}
	\vspace{3mm}
	\caption[Evaluation Topics using the Known Test Set]{The evaluation topics, the number of tagged test sentences and an example sentence for each topic in the known test set. It contains a total of \tt{584} tagged sentences, consisting of \tt{4009} word-tag tuples.}
	\label{t.evaluation_topics_known}
	\vspace{1em}
\end{table}

\begin{table}[!ht]
	\centering\small\def\arraystretch{1.2}\begin{tabular}{ p{18mm} c L{95mm} }
	\toprule
	\textsc{Topic} & \textsc{Sentences} & \multicolumn{1}{L{95mm}}{\textsc{Example}} \\
	\midrule
	\midrule
	ECTS & \tt{44} & \multicolumn{1}{L{95mm}}{\i{All modules with 6 ECTS}} \\
	\midrule
	Time & \tt{55} & \multicolumn{1}{L{95mm}}{\i{Courses that take place on Tuesday}} \\
	\midrule
	Faculty & \tt{56} & \multicolumn{1}{L{95mm}}{\i{Show all modules of faculty 4 to me}} \\
	\midrule
	Participants & \tt{54} & \multicolumn{1}{L{95mm}}{\i{List all courses that have a limitation of 30 participants}} \\
	\midrule
	Persons & \tt{58} & \multicolumn{1}{L{95mm}}{\i{Show all courses that are taught by Professor Martinelli}} \\
	\midrule
	Program & \tt{10} & \multicolumn{1}{L{95mm}}{\i{Show all modules of computer science }} \\
	\midrule
	Modules & \tt{60} & \multicolumn{1}{L{95mm}}{\i{Show information about the module Cognitive Algorithms}} \\
	\midrule
	Chair & \tt{20} & \multicolumn{1}{L{95mm}}{\i{All modules that the institute Media Science offers}} \\
	\midrule
	Exam & \tt{45} & \multicolumn{1}{L{95mm}}{\i{Show me all courses that have Group Lecture as examination}} \\
	\midrule
	Courses & \tt{30} & \multicolumn{1}{L{95mm}}{\i{The date of the first meeting of Internet Security }} \\
	\midrule
	Locations & \tt{42} & \multicolumn{1}{L{95mm}}{\i{All courses in auditorium Mainbuilding H 1012}} \\
	\bottomrule
	 & \tt{474} & \\
	\end{tabular}
	\vspace{3mm}
	\caption[Evaluation Topics using the Unknown Test Set]{The evaluation topics, the number of tagged test sentences and an example sentence for each topic in the unknown test set. It contains a total of \tt{474} tagged sentences, consisting of \tt{3669} word-tag tuples.}
	\label{t.evaluation_topics_unknown}
\end{table}

\section{Evaluation Results}\label{c.evaluation.results}
After introducing the test sets, the different training groups were evaluated according to the parameter configuration presented in chapter \ref{c.training.tuning}. The following sections show the evaluation results of the different architectures for each test set.

\subsection{Feed-Forward Neural Network Models}\label{c.evaluation.results.fnn}
...

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/evaluation_fnn_p}
	\caption[FNN Evaluation: Number of Past Words]{The evaluation results of the FNN for parameter \tt{p}: The number of preceding words.}
	\label{f.evaluation.fnn.p}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/evaluation_fnn_e}
	\caption[FNN Evaluation: Number of Past Words]{The evaluation results of the FNN for parameter \tt{e}: The embedding size.}
	\label{f.evaluation.fnn.e}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/evaluation_fnn_s}
	\caption[FNN Evaluation: Hidden Layer Size]{The evaluation results of the FNN for parameter \tt{s}: The size of the hidden layer.}
	\label{f.evaluation.fnn.s}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/evaluation_fnn_n}
	\caption[FNN Evaluation: Number of Training Epochs]{The evaluation results of the FNN for parameter \tt{n}: The number of training epochs.}
	\label{f.evaluation.fnn.n}
\end{figure}

\subsection{Recurrent Neural Network Models}\label{c.evaluation.results.rnn}
...

\subsection{Hidden Markov Models}\label{c.evaluation.results.hmm}
After evaluating the Neural Network based language models, now the HMM is evaluated as well. For this purpose the former HMM trained on the basis of the training data generated by T. Michael \cite{michael2016} and a new HMM trained on the basis of the training data proposed in this thesis are compared. The evaluation was performed with known and the unknown test and the results are shown in figure \ref{f.evaluation.hmm}.

\begin{figure}[H]
	\centering\includegraphics[width=.7\textwidth]{images/evaluation_hmm}
	\caption[HMM Evaluation]{The evaluation results of the former HMM trained by T. Michael \cite{michael2016} and the new HMM based on the new training data proposed in this thesis.}
	\label{f.evaluation.hmm}
\end{figure}

The former HMM correctly tagged \tt{82.5\%} of the words of the known test and \tt{63.0\%} of the unknown test, whereas the new HMM reached \tt{93.4\%} for the known and \tt{79.3\%} for the unknown test. Clearly the HMM based on the new training data performed significantly better than the former HMM. Especially the result for the unknown test is remarkable for the accuracy increases by more than \tt{16\%}.

It should be noted that the comparison of the former and the new HMM model based on the known test set is only partly representative, given the fact that the known test was not entirely known by the former HMM. Nevertheless the major part of known test set based on training templates, that were already used to build the corpus for the former HMM, which is why it still scored well with more than 4 out of 5 correct tags.

\section{Overall Comparison}\label{c.evaluation.comparison}
...

% ===================================================================================
\chapter{Discussion and Conclusion}\label{c.conclusion}
The final chapter of this thesis provides an outline of the work of this thesis and the achieved evaluation results. Findings from the evaluation are discussed and possible further research tasks based on this thesis are stated.

\section{Summary}\label{c.conclusion.summary}
The aim to develop a neural network based part-of-speech tagger for the advisory Artificial Conversational Agent \Alex\ was accomplished within this thesis.

A tagger module was implemented featuring a feed-forward Neural Network as well as a recurrent Neural Network. A training corpus containing tagged sentences was created with the help of an improved set of sentence templates. With both Neural Network architectures, various language models were trained on this corpus via parameter variation. These models were then evaluated with two corresponding tests sets, containing sentences from the training corpus (known data) and unknown sentences.

\section{Discussion}\label{c.conclusion.discussion}
...

\section{Future work}\label{c.conclusion.future}
Although a high accuracy of the language models was achieved with an improved training corpus and the Neural Network approaches in this thesis, further research could be done in specific areas.

Better than simply deleting training templates that contain a lot of useless combinations of data from the database, an additional module could check these combinations in advance. Depending on the query result of such a data validation module, the generated training sentence will be included or excluded from the training corpus. This way, the general sentence templates can still be used without worrying about meaningless training sentences.

In this thesis, parameters like the number of previous words, the size of the hidden layer, the size of the word embeddings and the number of training epochs were analyzed. Yet there are additional parameters, which can also be examined. For the FNN, possible parameters are the number of subsequent words (in addition to the number of previous words), the number of hidden layers, the type of the activation function for the neurons and the training optimizer. The RNN can utilize word embeddings with a particular embedding size too and the type of the activation function as well as the optimizer are possible additional parameters. Also an exponential decay for the learning rate of the optimizer especially for the RNN could be studied.

When using word embeddings, another possible improvement is the use of pretrained word embeddings instead of randomly initialized ones. It was previously shown that word embeddings that were trained on large German text corpora contain a high level of syntactical and semantical information \cite{mueller2015}.

Furthermore the evaluation results could be extended with a more fine-grained output concerning semantical topics like degrees, locations, persons or module titles. That way a statement can be made about how well the model performs on each topic and if there are significant differences between the evaluation results. This knowledge can than be used to systematically improve the training templates.

% ===================================================================================
\bibliographystyle{plain}
\bibliography{bibliography}

% ===================================================================================
\appendix
\chapter{Appendix}\label{c.appendix}

\section{Set of sentence templates}\label{c.appendix.sentencetemplates}
\lstinputlisting[language=plain, label={l.trainingtemplate}]{listings/training_template.txt}

\end{document}