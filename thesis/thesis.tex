\input{thesis_preamble.tex} % import preamble config
% start document
\begin{document}
\pagenumbering{arabic} % große Römische Seitenummerierung
\pagestyle{empty}

% title page
\clearscrheadings\clearscrplain
\begin{center}
\includegraphics[width=0.28\textwidth]{images/logo_tu_berlin}
\vspace{8mm}

{\huge Technische Universität Berlin}\\
\vspace{2mm}
{\large Quality and Usability Lab}\\
% \vspace{1mm}
% {\large Institute of Software Engineering\\and Theoretical Computer Science}\\
\vspace{11mm}

{\Huge Part-of-Speech Tagging\\[-2mm] with Neural Networks\\[-2mm] for a Conversational Agent\\}
\vspace{20mm}
{\Huge \b{Master Thesis}}\\
{\b{Master of Science (M.Sc.)}}\\
\vspace{24mm}
\begin{tabular}{rl}
  \b{Author} & Andreas Müller\\
  \b{Major} & Computer Engineering\\
  \b{Matriculation No.} & 333471\\
   & \\
  \b{Date} & 18th May 2018 \\
  \b{1st supervisor} & Prof. Dr.-Ing. Sebastian Möller \\
  \b{2nd supervisor} & Prof. Dr. ??? \\
\end{tabular}

\end{center}
\clearpage
\pagestyle{scrheadings} % normale Kopf- und Fußzeilen für den Rest

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Eidesstattliche Erklärung}
Hiermit versichere ich, dass ich die vorliegende Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Ausführungen, die anderen veröffentlichten oder nicht veröffentlichten Schriften wörtlich oder sinngemäß entnommen wurden, habe ich kenntlich gemacht.

Die Arbeit hat in gleicher oder ähnlicher Fassung noch keiner anderen Prüfungsbehörde vorgelegen.
\vspace{10mm}

Berlin, den \today\\

\vspace{1cm}
\rule{.5\textwidth}{.5pt}\\
Unterschrift

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Abstract}
A part-of-speech tagger is a system which automatically assigns the part of speech to words using contextual information. Potential applications for part-of-speech taggers exist in many areas of computational linguistics including speech recognition, speech synthesis, machine translation or information retrieval in general.

The part-of-speech tagging task of natural language processing is also used in the advisory artificial conversational agent called \Alex. \Alex\ was developed to answer questions about modules and courses at the Technische Universität Berlin. The system takes the written natural language requests from the user and tries to transform them into SQL-queries. To understand the natural language queries, the system uses a Hidden Markov Model (HMM) to assign tags to each word of the query (part-of-speech tagging). This HMM tagger is trained with manually created training templates that are filled with the data in the database to be queried. The manually created sentence-templates and the slot-filling resulted in many training data sentences with the same structure. This often led to wrong tagging results when the HMM tagger was presented with an input sentence, having a structure that doesn't occur in the training templates.

This thesis shows two different neural network approaches for the language modeling of the input sentences and evaluates and compares both neural network based tagger as well as the HMM based tagger.

% ===================================================================================
\BlankPage

% ===================================================================================
\chapter*{Zusammenfassung}
Ein Part-of-speech Tagger ist ein System, welches Wortarten anhand von Kontextinformationen automatisch den gegebenen Wörtern zuordnet. Potentielle Anwendungen solcher Tagger gibt es in vielen Bereichen der Computerlinguistik wie Spracherkennung, Sprachsynthese, maschinelle Übersetzung oder Information Retrieval im Allgemeinen.

Part-of-speech Tagging wird auch in \Alex\ verwendet, einem Artificial Conversational Agent. \Alex\ wurde entwickelt, um Fragen zu Modulen und Lehrveranstaltungen an der Technischen Universität Berlin zu beantworten. Das System nimmt die in natürlicher Sprache geschriebenen Anfragen des Benutzers und versucht diese in SQL-Abfragen umzuwandeln. Um die natürliche Sprache zu verstehen, verwendet das System ein Hidden-Markov-Model (HMM), um jedem Wort der Eingabe Wortarten zuzuweisen (Part-of-speech Tagging). Dieser HMM-Tagger wird mit manuell erstellten Trainingsvorlagen trainiert, die mit den Daten der abzufragenden Datenbank gefüllt werden. Die manuell erstellten Satzvorlagen führten zu vielen Trainingsdatensätzen mit gleicher Struktur und damit oft zu falschen Tagging-Ergebnissen, wenn der HMM-Tagger einen Eingabesatz mit einer Struktur verarbeiten sollte, die in den Trainingsvorlagen nicht vorkommt.

Diese Arbeit zeigt zwei verschiedene Ansätze für die Sprachmodellierung der Eingabesätze basierend auf neuronalen Netzwerken und bewertet und vergleicht sowohl die Neuronalen Netzwerk-basierten Tagger als auch den HMM-basierten Tagger.

% ===================================================================================
\BlankPage

% ===================================================================================
\tableofcontents

% ===================================================================================
\listoffigures

% ===================================================================================
\listoftables

% ===================================================================================
\chapter*{Abbreviations}\label{s.abbr}
\addcontentsline{toc}{chapter}{Abbreviations}
\markboth{Abbreviations}{Abbreviations}
\begin{acronym}[----------------]
 \acro{ACA}{\i{Artificial Conversational Agent}}
 \acro{FNN}{\i{Feed-forward Neural Network}}
 \acro{HMM}{\i{Hidden Markov Model}}
 \acro{NLP}{\i{Natural Language Processing}}
 \acro{NLTK}{\i{Natural Language Toolkit}}
 \acro{RNN}{\i{Recurrent Neural Network}}
\end{acronym}

% ===================================================================================
\chapter{Introduction}\label{c.introduction}
% Turing: Artificial Intelligence
% Brown: NLP
% POS Tagging

\section{Scope of this Thesis}\label{c.introduction.scope}
The scope of this thesis is the development of a neural network based part-of-speech tagger for the advisory Artificial Conversational Agent (ACA) \Alex, the training of different language models and their evaluation with corresponding test sets.

In order to accomplish the new language models, two different neural network architectures are implemented: A feed-forward neural network and a recurrent neural network. For the training of both neural network implementations, a corpus of tagged language data is generated with the help various input templates, which are created on the basis of logged user input data.

To evaluate the language models, a data set of known data\footnote{Data, that was already used for the training of the model} and unknown data\footnote{Data, that includes words and sentence structures, that didn't occur in the training data sets} is created. On the basis of this evaluation, both neural network models and the HMM are compared to each other.

In accordance to the evaluation results, the former HMM based part-of-speech tagger is then replaced by this new tagger. To guarantee a seamless integration, the new tagger is implemented as a separate module with the same program interface the old tagger already utilizes. This way no other components of the conversational agent have to be changed and the effort of the replacement is kept minimal.

\section{Related Work}\label{c.introduction.related}
...

\section{Structure of this Thesis}\label{c.introduction.structure}
As introduction, this first chapter gave a short overview about the subject of natural language processing and part-of-speech tagging in general.

The second chapter describes structure and functionality of the already existing ACA \Alex\ with the main focus on its language model and tagging interface.

Chapter \ref{c.postagging} explains the implementation of a part-of-speech tagging system with two different neural network approaches.

The training of the language models including the retrieval of the training data and tuning of the training parameter is described in Chapter \ref{c.training}.

Chapter \ref{c.evaluation} shows the evaluation of each language model with a generated test set and their comparison.

In conclusion the final Chapter \ref{c.conclusion} discusses and summarizes the evaluation results and gives an outlook on future work.

% ===================================================================================
\chapter{\Alex: Artificial Conversational Agent}\label{c.alex}
...

\section{System Overview}\label{c.alex.overview}
The modular structure of \Alex\ allows the separation of different functions and therefore easier replaceability of certain functionalities. Besides a web crawler for current data retrieval for the database and a frontend interface module, \Alex\ offers a tagging module. This module provides the training of a language model as well as the assignment of tags to the words of a given input sentence.

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/alex_components}
	\caption[Component Overview of \Alex]{Overview of all components of \Alex. The red colored parts are components that lie within the scope of this thesis and are adapted or replaced. Original figure by T. Michael \cite{michael2016}}
	\label{f.alex_components}
\end{figure}

\section{Hidden Markov Model}\label{c.alex.hmm}
The Hidden Markov Model (HMM) is a statistical tool that solves classification problems in general. It uses the observable output data of a system to derive hidden information from it. Among other applications, HMMs are used especially for speech recognition tasks.

The preliminary work for HMMs was done by R. L. Stratonovich. He first described the conditional Markov processes in 1960 \cite{stratonovich1960} that were used in the following years to describe simple Markov Models and later Hidden Markov Models (see Baum et. al. \cite{baum1966}\cite{baum1967}). The latter became popular for solving the task of automatic recognition of continuous speech \cite{baker1975} along with other applications like pattern recognition in general, the analysis of biological sequences (e.g. DNA) \cite{bishop1986} and part-of-speech tagging.

In case of POS tagging, the observable states of the HMM would be simply the given sequence of words whereas the hidden states represent the parts of speech. The HMM calculates the joint probability of the whole sequence of hidden states, finds a maximum of all possible state sequences and decides as a result, which parts of speech are most likely applied to the words of the input sequence.

For this purpose, an HMM is included in \Alex. According to T. Michael \cite{michael2016}, a tagging scheme was developed to extract exactly the information from a user input that is needed to create a successful database query and return the information the user asked for. This tagging scheme was intentionally built domain independent to make \Alex\ an ACA for any topic providing a corresponding database and training data.

To maintain this property, nothing is changed about the tagging scheme of \Alex. For a better understanding of the evaluation results in Chapter \ref{c.evaluation}, table \ref{t.tagging_scheme} gives an overview of the 6 different classes of tags that are used by \Alex.

\begin{table}[H]
	\vspace{1ex}\small\centering\def\arraystretch{1.1}\begin{tabular}{|c|>{\raggedright}p{50mm}|p{35mm}|p{40mm}|}
	\hline
	Class & Formats & Description & Example \\
	\hline\hline
	\tt{R} & \i{Return-tags}, describing data that is expected to be returned & \tt{R\_LIST} \newline \tt{R\_SINGLE} \newline \tt{R\_COUNT} & \i{"\b{Which} modules \dots"} \newline \i{"\b{Which} module \dots"} \newline \i{"\b{How many} modules \dots"} \\ \hline
	\tt{M} & \i{Model-tags}, describing the database model, e.g. \tt{M\_MTSModule} or \tt{M\_Course} & \tt{M\_[MODEL]} & \i{"Which \b{modules} \dots"} \newline \i{"Which \b{courses} \dots"} \\ \hline
	\tt{C} & \i{Constraint-tags}, filtering the result set, given a database model and corresponding field, e.g. \tt{C\_MTSModule:ects} & \tt{C\_[MODEL]:[FIELD]} & \i{"Modules with \b{6} ects \dots"} \\ \hline
	\tt{P} & \i{Property-tags}, indicating to include fields in the result set, e.g. \tt{P\_MTSModule:ects} & \tt{P\_[MODEL]:[FIELD]} & \i{"Modules with 6 \b{ects} \dots"} \\ \hline
	\tt{Q} & \i{Comparison-tags}, describing an equal, greater than or less than constraint & \tt{Q\_EQ} \newline \tt{Q\_LT} \newline \tt{Q\_GT} &  \i{"\dots\ with \b{exactly} 6 ects \dots"} \newline \i{"\dots\ \b{less than} 6 ects \dots"} \newline \i{"\dots\ \b{more than} 6 ects \dots"} \\ \hline
	\tt{X} & \i{Extra-tags}, describing words that are not relevant for the database query\tablefootnote{This can be either words with no special meaning at all (tagged with \tt{X}), or words that have no meaning for the database query but for the system itself (e.g. the tag \tt{X\_HELP} for the word \i{"help"}) or words that lead to a particular constraint (like the tag \tt{X\_Person:fullname} for the word \i{"Professor"}, that leads to a name).} & \tt{X} \newline \tt{X\_[WORD]} \newline \tt{X\_[MODEL]:[FIELD]} & \i{"\b{and}"}, \i{"\b{of}"}, \i{"\b{is}"} \newline \i{"I need \b{help}"} \newline \i{"\b{Professor} John Doe"} \\ \hline
	\end{tabular}
	\caption[Tagging Scheme Overview]{Overview of the tagging scheme used in \Alex, consisting of 6 different classes of tags with a total of 12 different formats. The examples contain \b{emphasized} words that belong to the corresponding tag format. Detailed explanation of the tagging classes and its formats is given by T. Michael \cite{michael2016}.}
	\label{t.tagging_scheme}
	\vspace{1ex}
\end{table}

\section{Tagging Interface}\label{c.alex.tagging}
As described in the previous chapter, the implementation of the tagging module of \Alex\ utilizes a Hidden Markov Model for the part-of-speech tagging. \Alex\ uses an already existing implementation of the HMM Tagger from the Natural Language Toolkit (NLTK)\footnote{The Natural Language Toolkit is a collection of \i{Python} programming libraries for natural language processing, see \link{http://nltk.org}}, called \tt{HiddenMarkovModelTagger}.

To replace the existing tagger, a new tagger has to provide a class with two methods: \tt{train} and \tt{tag}. These methods are used to create the language model and apply it to unknown data.

The \tt{train} method creates a new instance of the tagger class, trains this class with the given training data and returns it. The training data itself must be a list of sentences, where a sentence is a list of tuples, containing each word of this sentence and its corresponding tag. The following exemplifies the structure of the training input data containing two sentences where each word is tagged with \i{TAG}:

\lstinputlisting[language=JSON, label={l.trainingdata}]{listings/method-train.example}

The \tt{tag} method attaches a tag to each word of an input sentence, according to the previously trained language model. The input has to be an unknown sentence as a simple list of words:

\lstinputlisting[language=JSON]{listings/method-tag-input.example}

The output is a corresponding list of tuples containing a word and its assigned tag:

\lstinputlisting[language=JSON]{listings/method-tag-output.example}


% ===================================================================================
\chapter{Part-of-Speech Tagging}\label{c.postagging}
...
Ma et. al. have shown before, that the neural network approach outperforms part-of-speech tagger that are based on statistical models (like the HMM) \cite{ma2000}.

\section{Feed-forward Neural Network Model}\label{c.postagging.fnn}
...

\subsection{Architecture}\label{c.postagging.fnn.architecture}
...

\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/fnn_structure}
	\caption[Structure of a Feed-forward Neural Network]{The structure of a feed-forward neural network. The feature vector is built by the initial vectors of the corresponding input word on position \tt{p} and by its three predecessors (here as an example).}
	\label{f.fnn_structure}
\end{figure}

\subsection{Implementation}\label{c.postagging.fnn.implementation}
...

\section{Recurrent Neural Network Model}\label{c.postagging.rnn}
...

\subsection{Architecture}\label{c.postagging.rnn.architecture}
...
\begin{figure}[H]
	\includegraphics[width=\textwidth]{images/rnn_structure}
	\caption[Structure of a Recurrent Neural Network]{The structure of a recurrent neural network. The feature vector is the initial vector of the corresponding input word at time \tt{t}. The output of the hidden layer from previously trained words (here as an example at time \tt{x}) is fed back into the same hidden layer for the current word.}
	\label{f.rnn_structure}
\end{figure}

\subsection{Implementation}\label{c.postagging.rnn.implementation}
...

% ===================================================================================
\chapter{Training}\label{c.training}
...

\section{Data Retrieval}\label{c.training.data}
...

\section{Parameter Tuning}\label{c.training.tuning}
...

% ===================================================================================
\chapter{Evaluation and Comparison}\label{c.evaluation}
...

\section{Test Design}\label{c.evaluation.test}
...

% ===================================================================================
\chapter{Discussion and Conclusion}\label{c.conclusion}
...

\section{Summary}\label{c.conclusion.summary}
...

\section{Discussion}\label{c.conclusion.discussion}
...

\section{Future work}\label{c.conclusion.future}
...
\cite{michael2016}
\cite{mueller2015}
\cite{ma2000}
\cite{schmid1994}

% ===================================================================================
% \bibliographystyle{apalike}
% \bibliographystyle{plainnat}
% \bibliographystyle{abbrvnat}
% \bibliographystyle{unsrtnat}
% \bibliographystyle{alphadin}
\bibliographystyle{plain}
\bibliography{bibliography}

% ===================================================================================
\appendix
\chapter{First appendix}

\section{test}
...

\end{document}